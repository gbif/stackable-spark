---
apiVersion: spark.stackable.tech/v1alpha1
kind: SparkApplication
metadata:
  name: occurrence-table-build
spec:
  roleGroups:
    default:
      podOverrides:
        metadata:
          labels:
            my-custom-label: super-important-label
  version: "1.0"
  sparkImage: docker.stackable.tech/stackable/spark-k8s:3.3.0-stackable0.1.0
  mode: cluster
  mainApplicationFile: hdfs://gbif-hdfs-namenode-default-0/apps/occurrence-table-build-spark.jar
  mainClass: org.gbif.occurrence.table.backfill.TableBackfill
  args:
    - "/etc/gbif/occurrence-table-backfill.yml"
    - "CREATE"
    - "TABLE"
  sparkConf:
    "spark.driver.extraJavaOptions": "-XX:+UseConcMarkSweepGC"
    "spark.executor.extraJavaOptions": "-XX:+UseConcMarkSweepGC"
    "spark.jars.ivy": "/tmp"
    "spark.broadcast.compress": "true"
    "spark.checkpoint.compress": "true"
    "spark.executor.memoryOverhead": "2048"
    "spark.executor.heartbeatInterval": "20s"
    "spark.network.timeout": "240s"
    "spark.io.compression.codec": "snappy"
    "spark.rdd.compress": "true"
    "spark.driver.userClassPathFirst" : "true"
    "spark.driver.extraClassPath": "/etc/hadoop/conf/:/stackable/spark/extra-jars/*"
    "spark.executor.extraClassPath": "/etc/hadoop/conf/:/stackable/spark/extra-jars/*"
    "spark.kubernetes.authenticate.driver.serviceAccountName": "gbif-spark-sa"
    "spark.kubernetes.scheduler.name": "yunikorn"
    "spark.kubernetes.driver.label.queue": "root.default"
    "spark.kubernetes.executor.label.queue": "root.default"
    "spark.kubernetes.driver.annotation.yunikorn.apache.org/app-id": "{{APP_ID}}"
    "spark.kubernetes.executor.annotation.yunikorn.apache.org/app-id": "{{APP_ID}}"
  # The following config maps are managed in our K8s environment using GBIF naming convention
  volumes:
    - name: hadoop-env
      configMap:
        name: gbif-hdfs
        items:
          - key: core-site.xml
            path: core-site.xml
          - key: hdfs-site.xml
            path: hdfs-site.xml
    - name: hive-env
      configMap:
        name: gbif-hive-metastore-metastore-default
        items:
          - key: hive-site.xml
            path: hive-site.xml
    - name: occurrence-config
      configMap:
        name: gbif-occurrence-table-build-env
        items:
          - key: occurrence-table-backfill.yml
            path: occurrence-table-backfill.yml
  driver:
    podOverrides:
      metadata:
        annotations:
          yunikorn.apache.org/task-group-name: "spark-driver"
          yunikorn.apache.org/task-groups: |-
            [{
              "name": "spark-driver",
              "minMember": 1,
              "minResource": {
                "cpu": "1",
                "memory": "1Gi"
              }
            }]
    config:
      resources:
        cpu:
          min: "200m"
          max: "2000m"
        memory:
          limit: "1Gi"
      # Mount the GBIF-managed volumes providing environment configuration for Spark
      volumeMounts:
        - name: hadoop-env
          mountPath: /etc/hadoop/conf/core-site.xml
          subPath: core-site.xml
        - name: hadoop-env
          mountPath: /etc/hadoop/conf/hdfs-site.xml
          subPath: hdfs-site.xml
        - name: hive-env
          mountPath: /etc/hadoop/conf/hive-site.xml
          subPath: hive-site.xml
        - name: hbase-env
          mountPath: /etc/hadoop/conf/hbase-site.xml
          subPath: hbase-site.xml
        - name: pipelines-conf
          mountPath: /etc/pipelines/conf/metrics.properties
          subPath: metrics.properties
  executor:
    podOverrides:
      metadata:
        annotations:
          yunikorn.apache.org/task-group-name: "spark-executor"
          yunikorn.apache.org/task-groups: |-
            [{
              "name": "spark-executor",
              "minMember": 1,
              "minResource": {
                "cpu": "4000m",
                "memory": "2Gi"
              }
            }]
    replicas: 10
    config:
      resources:
        cpu:
          min: "3000m"
          max: "5000m"
        memory:
          limit: "4Gi"
      # Mount the GBIF-managed volumes providing environment configuration for Spark
      volumeMounts:
        - name: hadoop-env
          mountPath: /etc/hadoop/conf/core-site.xml
          subPath: core-site.xml
        - name: hadoop-env
          mountPath: /etc/hadoop/conf/hdfs-site.xml
          subPath: hdfs-site.xml
        - name: hbase-env
          mountPath: /etc/hadoop/conf/hbase-site.xml
          subPath: hbase-site.xml
        - name: hive-env
          mountPath: /etc/hadoop/conf/hive-site.xml
          subPath: hive-site.xml
