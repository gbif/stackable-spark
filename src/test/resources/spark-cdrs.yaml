---
apiVersion: spark.stackable.tech/v1alpha1
kind: SparkApplication
metadata:
  name: spark-pi
spec:
  version: "1.0"
  sparkImage:
    productVersion: 3.4.0
    stackableVersion: 23.11.0
  mode: cluster
  mainClass: org.apache.spark.examples.SparkPi
  mainApplicationFile: /stackable/spark/examples/jars/spark-examples_2.12-3.3.0.jar
  executor:
    replicas: 3
---
apiVersion: spark.stackable.tech/v1alpha1
kind: SparkApplication
metadata:
  name: example-sparkapp-pvc
  namespace: default
spec:
  version: "1.0"
  sparkImage:
    productVersion: 3.4.0
    stackableVersion: 23.11.0
  mode: cluster
  mainApplicationFile: s3a://stackable-spark-k8s-jars/jobs/ny-tlc-report-1.0-SNAPSHOT.jar
  mainClass: org.example.App
  args:
    - "'s3a://nyc-tlc/trip data/yellow_tripdata_2021-07.csv'"
  sparkConf:
    "spark.hadoop.fs.s3a.aws.credentials.provider": "org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider"
    "spark.driver.extraClassPath": "/dependencies/jars/*"
    "spark.executor.extraClassPath": "/dependencies/jars/*"
  volumes:
    - name: job-deps
      persistentVolumeClaim:
        claimName: pvc-ksv
  driver:
    config:
      volumeMounts:
        - name: job-deps
          mountPath: /dependencies
  executor:
    replicas: 3
    config:
      volumeMounts:
        - name: job-deps
          mountPath: /dependencies
---
apiVersion: spark.stackable.tech/v1alpha1
kind: SparkApplication
metadata:
  name: example-sparkapp-external-dependencies
  namespace: default
spec:
  version: "1.0"
  sparkImage:
    productVersion: 3.4.0
    stackableVersion: 23.11.0
  mode: cluster
  mainApplicationFile: s3a://stackable-spark-k8s-jars/jobs/ny_tlc_report.py
  args:
    - "--input 's3a://nyc-tlc/trip data/yellow_tripdata_2021-07.csv'"
  deps:
    requirements:
      - tabulate==0.8.9
  sparkConf:
    "spark.hadoop.fs.s3a.aws.credentials.provider": "org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider"
    "spark.driver.extraClassPath": "/dependencies/jars/*"
    "spark.executor.extraClassPath": "/dependencies/jars/*"
  volumes:
    - name: job-deps
      persistentVolumeClaim:
        claimName: pvc-ksv
  driver:
    config:
      volumeMounts:
        - name: job-deps
          mountPath: /dependencies
  executor:
    replicas: 3
    config:
      volumeMounts:
        - name: job-deps
          mountPath: /dependencies
---
apiVersion: spark.stackable.tech/v1alpha1
kind: SparkApplication
metadata:
  name: example-sparkapp-external-dependencies
  namespace: default
spec:
  version: "1.0"
  sparkImage:
    productVersion: 3.4.0
    stackableVersion: 23.11.0
  mode: cluster
  mainApplicationFile: s3a://stackable-spark-k8s-jars/jobs/ny_tlc_report.py
  args:
    - "--input 's3a://nyc-tlc/trip data/yellow_tripdata_2021-07.csv'"
  deps:
    requirements:
      - tabulate==0.8.9
  sparkConf:
    "spark.hadoop.fs.s3a.aws.credentials.provider": "org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider"
    "spark.driver.extraClassPath": "/dependencies/jars/*"
    "spark.executor.extraClassPath": "/dependencies/jars/*"
  volumes:
    - name: job-deps
      persistentVolumeClaim:
        claimName: pvc-ksv
  driver:
    config:
      volumeMounts:
        - name: job-deps
          mountPath: /dependencies
  executor:
    replicas: 3
    config:
      volumeMounts:
        - name: job-deps
          mountPath: /dependencies
---
apiVersion: spark.stackable.tech/v1alpha1
kind: SparkApplication
metadata:
  name: example-sparkapp-image
  namespace: default
spec:
  version: "1.0"
  image: docker.stackable.tech/stackable/ny-tlc-report:0.1.0
  sparkImage:
    productVersion: 3.4.0
    stackableVersion: 23.11.0
  mode: cluster
  mainApplicationFile: local:///stackable/spark/jobs/ny_tlc_report.py
  args:
    - "--input 's3a://nyc-tlc/trip data/yellow_tripdata_2021-07.csv'"
  deps:
    requirements:
      - tabulate==0.8.9
  sparkConf:
    "spark.hadoop.fs.s3a.aws.credentials.provider": "org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider"
  job:
    config:
      resources:
        cpu:
          min: "1"
          max: "1"
        memory:
          limit: "1Gi"
  driver:
    config:
      resources:
        cpu:
          min: "1"
          max: "1500m"
        memory:
          limit: "1Gi"
  executor:
    replicas: 3
    config:
      resources:
        cpu:
          min: "1"
          max: "4"
        memory:
          limit: "2Gi"
---
apiVersion: spark.stackable.tech/v1alpha1
kind: SparkApplication
metadata:
  name: example-sparkapp-pvc
  namespace: default
spec:
  version: "1.0"
  sparkImage:
    productVersion: 3.4.0
    stackableVersion: 23.11.0
  mode: cluster
  mainApplicationFile: s3a://stackable-spark-k8s-jars/jobs/ny-tlc-report-1.0-SNAPSHOT.jar
  mainClass: org.example.App
  args:
    - "'s3a://nyc-tlc/trip data/yellow_tripdata_2021-07.csv'"
  sparkConf:
    "spark.hadoop.fs.s3a.aws.credentials.provider": "org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider"
    "spark.driver.extraClassPath": "/dependencies/jars/*"
    "spark.executor.extraClassPath": "/dependencies/jars/*"
  volumes:
    - name: job-deps
      persistentVolumeClaim:
        claimName: pvc-ksv
  driver:
    config:
      volumeMounts:
        - name: job-deps
          mountPath: /dependencies
  executor:
    replicas: 3
    config:
      volumeMounts:
        - name: job-deps
          mountPath: /dependencies
---
apiVersion: spark.stackable.tech/v1alpha1
kind: SparkApplication
metadata:
  name: occurrence-table-build
spec:
  roleGroups:
    default:
      podOverrides:
        metadata:
          labels:
            my-custom-label: super-important-label
  version: "1.0"
  sparkImage:
    productVersion: 3.4.0
    stackableVersion: 23.11.0
  mode: cluster
  mainApplicationFile: hdfs://gbif-hdfs-namenode-default-0/apps/occurrence-table-build-spark.jar
  mainClass: org.gbif.occurrence.table.backfill.TableBackfill
  args:
    - "/etc/gbif/occurrence-table-backfill.yml"
    - "CREATE"
    - "TABLE"
  sparkConf:
    "spark.driver.extraJavaOptions": "-XX:+UseConcMarkSweepGC"
    "spark.executor.extraJavaOptions": "-XX:+UseConcMarkSweepGC"
    "spark.broadcast.compress": "true"
    "spark.checkpoint.compress": "true"
    "spark.executor.memoryOverhead": "4096"
    "spark.executor.heartbeatInterval": "20s"
    "spark.network.timeout": "240s"
    "spark.io.compression.codec": "lz4"
    "spark.rdd.compress": "true"
    "spark.driver.extraClassPath": "/dependencies/jars/*"
    "spark.executor.extraClassPath": "/dependencies/jars/*"
    "spark.driver.extraClassPath": "/etc/hadoop/conf/"
    "spark.executor.extraClassPath": "/etc/hadoop/conf/"
  # The following config maps are managed in our K8s environment using GBIF naming convention
  volumes:
    - name: hadoop-env
      configMap:
        name: gbif-hdfs
        items:
          - key: core-site.xml
            path: core-site.xml
          - key: hdfs-site.xml
            path: hdfs-site.xml
    - name: hive-env
      configMap:
        name: gbif-hive-metastore-metastore-default
        items:
          - key: hive-site.xml
            path: hive-site.xml
    - name: occurrence-config
      configMap:
        name: gbif-occurrence-table-build-env
        items:
          - key: occurrence-table-backfill.yml
            path: occurrence-table-backfill.yml
  driver:
    podOverrides:
      metadata:
        annotations:
          yunikorn.apache.org/task-group-name: "spark-driver"
          yunikorn.apache.org/task-groups: |-
            [{
              "name": "spark-driver",
              "minMember": 1,
              "minResource": {
                "cpu": "1",
                "memory": "1Gi"
              }
            }]
    config:
      resources:
        cpu:
          min: "100m"
          max: "1"
        memory:
          limit: "1Gi"
      logging:
        enableVectorAgent: true
        containers:
          vector:
            file:
              level: "WARN"
          spark:
            console:
              level: "INFO"
            file:
              level: "INFO"
            loggers:
              ROOT:
                level: "INFO"
      volumeMounts:
        - name: hadoop-env
          mountPath: /etc/hadoop/conf/core-site.xml
          subPath: core-site.xml
        - name: hadoop-env
          mountPath: /etc/hadoop/conf/hdfs-site.xml
          subPath: hdfs-site.xml
        - name: hive-env
          mountPath: /etc/hadoop/conf/hive-site.xml
          subPath: hive-site.xml
        - name: hbase-env
          mountPath: /etc/hadoop/conf/hbase-site.xml
          subPath: hbase-site.xml
        - name: pipelines-conf
          mountPath: /etc/pipelines/conf/metrics.properties
          subPath: metrics.properties
  executor:
    podOverrides:
      metadata:
        annotations:
          yunikorn.apache.org/task-group-name: "spark-executor"
          yunikorn.apache.org/task-groups: |-
            [{
              "name": "spark-executor",
              "minMember": 1,
              "minResource": {
                "cpu": "1",
                "memory": "1Gi"
              }
            }]
    replicas: 1
    config:
      resources:
        cpu:
          min: "100m"
          max: "1"
        memory:
          limit: "1Gi"
      logging:
        enableVectorAgent: true
        containers:
          vector:
            file:
              level: "WARN"
          spark:
            console:
              level: "INFO"
            file:
              level: "INFO"
            loggers:
              ROOT:
                level: "INFO"
      volumeMounts:
        - name: hadoop-env
          mountPath: /etc/hadoop/conf/core-site.xml
          subPath: core-site.xml
        - name: hadoop-env
          mountPath: /etc/hadoop/conf/hdfs-site.xml
          subPath: hdfs-site.xml
        - name: hbase-env
          mountPath: /etc/hadoop/conf/hbase-site.xml
          subPath: hbase-site.xml
        - name: hive-env
          mountPath: /etc/hadoop/conf/hive-site.xml
          subPath: hive-site.xml
