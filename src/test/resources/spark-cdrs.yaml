---
apiVersion: spark.stackable.tech/v1alpha1
kind: SparkApplication
metadata:
  name: occurrence-table-build
spec:
  roleGroups:
    default:
      podOverrides:
        metadata:
          labels:
            my-custom-label: super-important-label
  version: "1.0"
  sparkImage: docker.stackable.tech/stackable/spark-k8s:3.3.0-stackable0.1.0
  mode: cluster
  mainApplicationFile: hdfs://gbif-hdfs-namenode-default-0/apps/occurrence-table-build-spark.jar
  mainClass: org.gbif.occurrence.table.backfill.TableBackfill
  args:
    - "/etc/gbif/occurrence-table-backfill.yml"
    - "CREATE"
    - "TABLE"
  sparkConf:
    "spark.driver.extraJavaOptions": "-XX:+UseConcMarkSweepGC"
    "spark.executor.extraJavaOptions": "-XX:+UseConcMarkSweepGC"
    "spark.broadcast.compress": "true"
    "spark.checkpoint.compress": "true"
    "spark.executor.memoryOverhead": "4096"
    "spark.executor.heartbeatInterval": "20s"
    "spark.network.timeout": "240s"
    "spark.io.compression.codec": "lz4"
    "spark.rdd.compress": "true"
    "spark.driver.extraClassPath": "/dependencies/jars/*"
    "spark.executor.extraClassPath": "/dependencies/jars/*"
    "spark.driver.extraClassPath": "/etc/hadoop/conf/"
    "spark.executor.extraClassPath": "/etc/hadoop/conf/"
  # The following config maps are managed in our K8s environment using GBIF naming convention
  volumes:
    - name: hadoop-env
      configMap:
        name: gbif-hdfs
        items:
          - key: core-site.xml
            path: core-site.xml
          - key: hdfs-site.xml
            path: hdfs-site.xml
    - name: hive-env
      configMap:
        name: gbif-hive-metastore-metastore-default
        items:
          - key: hive-site.xml
            path: hive-site.xml
    - name: occurrence-config
      configMap:
        name: gbif-occurrence-table-build-env
        items:
          - key: occurrence-table-backfill.yml
            path: occurrence-table-backfill.yml
  driver:
    podOverrides:
      metadata:
        annotations:
          yunikorn.apache.org/task-group-name: "spark-driver"
          yunikorn.apache.org/task-groups:
            [{
              "name": "spark-driver",
              "minMember": 1,
              "minResource": {
                "cpu": "1",
                "memory": "1Gi"
              }
            }]
    resources:
      cpu:
        min: "100m"
        max: "4000m"
      memory:
        limit: "2Gi"
    # Mount the GBIF-managed volumes providing environment configuration for Spark
    volumeMounts:
      - name: occurrence-config
        mountPath: /etc/gbif
      - name: hadoop-env
        mountPath: /etc/hadoop/conf/core-site.xml
        subPath: core-site.xml
      - name: hadoop-env
        mountPath: /etc/hadoop/conf/hdfs-site.xml
        subPath: hdfs-site.xml
      - name: hive-env
        mountPath: /etc/hadoop/conf/hive-site.xml
        subPath: hive-site.xml
  executor:
    podOverrides:
      metadata:
        annotations:
          yunikorn.apache.org/task-group-name: "spark-executor"
          yunikorn.apache.org/task-groups:
            [{
              "name": "spark-executor",
              "minMember": 1,
              "minResource": {
                "cpu": "1",
                "memory": "1Gi"
              }
            }]
    instances: 10
    resources:
      cpu:
        min: "100m"
        max: "10000m"
      memory:
        limit: "8Gi"
    # Mount the GBIF-managed volumes providing environment configuration for Spark
    volumeMounts:
      - name: hadoop-env
        mountPath: /etc/hadoop/conf/core-site.xml
        subPath: core-site.xml
      - name: hadoop-env
        mountPath: /etc/hadoop/conf/hdfs-site.xml
        subPath: hdfs-site.xml
      - name: hive-env
        mountPath: /etc/hadoop/conf/hive-site.xml
        subPath: hive-site.xml
