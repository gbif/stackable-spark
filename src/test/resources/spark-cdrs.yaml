---
apiVersion: spark.stackable.tech/v1alpha1
kind: SparkApplication
metadata:
  name: spark-pi
spec:
  version: "1.0"
  sparkImage: docker.stackable.tech/stackable/spark-k8s:3.3.0-stackable0.3.0
  mode: cluster
  mainClass: org.apache.spark.examples.SparkPi
  mainApplicationFile: /stackable/spark/examples/jars/spark-examples_2.12-3.3.0.jar
  executor:
    instances: 3
---
apiVersion: spark.stackable.tech/v1alpha1
kind: SparkApplication
metadata:
  name: example-sparkapp-pvc
  namespace: default
spec:
  version: "1.0"
  sparkImage: docker.stackable.tech/stackable/spark-k8s:3.3.0-stackable0.3.0
  mode: cluster
  mainApplicationFile: s3a://stackable-spark-k8s-jars/jobs/ny-tlc-report-1.0-SNAPSHOT.jar
  mainClass: org.example.App
  args:
    - "'s3a://nyc-tlc/trip data/yellow_tripdata_2021-07.csv'"
  sparkConf:
    "spark.hadoop.fs.s3a.aws.credentials.provider": "org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider"
    "spark.driver.extraClassPath": "/dependencies/jars/*"
    "spark.executor.extraClassPath": "/dependencies/jars/*"
  volumes:
    - name: job-deps
      persistentVolumeClaim:
        claimName: pvc-ksv
  driver:
    volumeMounts:
      - name: job-deps
        mountPath: /dependencies
  executor:
    instances: 3
    volumeMounts:
      - name: job-deps
        mountPath: /dependencies
---
apiVersion: spark.stackable.tech/v1alpha1
kind: SparkApplication
metadata:
  name: example-sparkapp-external-dependencies
  namespace: default
spec:
  version: "1.0"
  sparkImage: docker.stackable.tech/stackable/pyspark-k8s:3.3.0-stackable0.3.0
  mode: cluster
  mainApplicationFile: s3a://stackable-spark-k8s-jars/jobs/ny_tlc_report.py
  args:
    - "--input 's3a://nyc-tlc/trip data/yellow_tripdata_2021-07.csv'"
  deps:
    requirements:
      - tabulate==0.8.9
  sparkConf:
    "spark.hadoop.fs.s3a.aws.credentials.provider": "org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider"
    "spark.driver.extraClassPath": "/dependencies/jars/*"
    "spark.executor.extraClassPath": "/dependencies/jars/*"
  volumes:
    - name: job-deps
      persistentVolumeClaim:
        claimName: pvc-ksv
  driver:
    volumeMounts:
      - name: job-deps
        mountPath: /dependencies
  executor:
    instances: 3
    volumeMounts:
      - name: job-deps
        mountPath: /dependencies
---
apiVersion: spark.stackable.tech/v1alpha1
kind: SparkApplication
metadata:
  name: example-sparkapp-external-dependencies
  namespace: default
spec:
  version: "1.0"
  sparkImage: docker.stackable.tech/stackable/pyspark-k8s:3.3.0-stackable0.3.0
  mode: cluster
  mainApplicationFile: s3a://stackable-spark-k8s-jars/jobs/ny_tlc_report.py
  args:
    - "--input 's3a://nyc-tlc/trip data/yellow_tripdata_2021-07.csv'"
  deps:
    requirements:
      - tabulate==0.8.9
  sparkConf:
    "spark.hadoop.fs.s3a.aws.credentials.provider": "org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider"
    "spark.driver.extraClassPath": "/dependencies/jars/*"
    "spark.executor.extraClassPath": "/dependencies/jars/*"
  volumes:
    - name: job-deps
      persistentVolumeClaim:
        claimName: pvc-ksv
  driver:
    volumeMounts:
      - name: job-deps
        mountPath: /dependencies
  executor:
    instances: 3
    volumeMounts:
      - name: job-deps
        mountPath: /dependencies
---
apiVersion: spark.stackable.tech/v1alpha1
kind: SparkApplication
metadata:
  name: example-sparkapp-image
  namespace: default
spec:
  version: "1.0"
  image: docker.stackable.tech/stackable/ny-tlc-report:0.1.0
  sparkImage: docker.stackable.tech/stackable/pyspark-k8s:3.3.0-stackable0.3.0
  mode: cluster
  mainApplicationFile: local:///stackable/spark/jobs/ny_tlc_report.py
  args:
    - "--input 's3a://nyc-tlc/trip data/yellow_tripdata_2021-07.csv'"
  deps:
    requirements:
      - tabulate==0.8.9
  sparkConf:
    "spark.hadoop.fs.s3a.aws.credentials.provider": "org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider"
  job:
    resources:
      cpu:
        min: "1"
        max: "1"
      memory:
        limit: "1Gi"
  driver:
    resources:
      cpu:
        min: "1"
        max: "1500m"
      memory:
        limit: "1Gi"
  executor:
    instances: 3
    resources:
      cpu:
        min: "1"
        max: "4"
      memory:
        limit: "2Gi"
---
apiVersion: spark.stackable.tech/v1alpha1
kind: SparkApplication
metadata:
  name: example-sparkapp-pvc
  namespace: default
spec:
  version: "1.0"
  sparkImage: docker.stackable.tech/stackable/spark-k8s:3.3.0-stackable0.3.0
  mode: cluster
  mainApplicationFile: s3a://stackable-spark-k8s-jars/jobs/ny-tlc-report-1.0-SNAPSHOT.jar
  mainClass: org.example.App
  args:
    - "'s3a://nyc-tlc/trip data/yellow_tripdata_2021-07.csv'"
  sparkConf:
    "spark.hadoop.fs.s3a.aws.credentials.provider": "org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider"
    "spark.driver.extraClassPath": "/dependencies/jars/*"
    "spark.executor.extraClassPath": "/dependencies/jars/*"
  volumes:
    - name: job-deps
      persistentVolumeClaim:
        claimName: pvc-ksv
  driver:
    volumeMounts:
      - name: job-deps
        mountPath: /dependencies
  executor:
    instances: 3
    volumeMounts:
      - name: job-deps
        mountPath: /dependencies
---
apiVersion: spark.stackable.tech/v1alpha1
kind: SparkApplication
metadata:
  name: occurrence-table-build
spec:
  roleGroups:
    default:
      podOverrides:
        metadata:
          labels:
            my-custom-label: super-important-label
  version: "1.0"
  sparkImage: docker.stackable.tech/stackable/spark-k8s:3.3.0-stackable0.1.0
  mode: cluster
  mainApplicationFile: hdfs://gbif-hdfs-namenode-default-0/apps/occurrence-table-build-spark.jar
  mainClass: org.gbif.occurrence.table.backfill.TableBackfill
  args:
    - "/etc/gbif/occurrence-table-backfill.yml"
    - "CREATE"
    - "TABLE"
  sparkConf:
    "spark.driver.extraJavaOptions": "-XX:+UseConcMarkSweepGC"
    "spark.executor.extraJavaOptions": "-XX:+UseConcMarkSweepGC"
    "spark.broadcast.compress": "true"
    "spark.checkpoint.compress": "true"
    "spark.executor.memoryOverhead": "4096"
    "spark.executor.heartbeatInterval": "20s"
    "spark.network.timeout": "240s"
    "spark.io.compression.codec": "lz4"
    "spark.rdd.compress": "true"
    "spark.driver.extraClassPath": "/dependencies/jars/*"
    "spark.executor.extraClassPath": "/dependencies/jars/*"
    "spark.driver.extraClassPath": "/etc/hadoop/conf/"
    "spark.executor.extraClassPath": "/etc/hadoop/conf/"
  # The following config maps are managed in our K8s environment using GBIF naming convention
  volumes:
    - name: hadoop-env
      configMap:
        name: gbif-hdfs
        items:
          - key: core-site.xml
            path: core-site.xml
          - key: hdfs-site.xml
            path: hdfs-site.xml
    - name: hive-env
      configMap:
        name: gbif-hive-metastore-metastore-default
        items:
          - key: hive-site.xml
            path: hive-site.xml
    - name: occurrence-config
      configMap:
        name: gbif-occurrence-table-build-env
        items:
          - key: occurrence-table-backfill.yml
            path: occurrence-table-backfill.yml
  driver:
    resources:
      cpu:
        min: "100m"
        max: "4000m"
      memory:
        limit: "2Gi"
    # Mount the GBIF-managed volumes providing environment configuration for Spark
    volumeMounts:
      - name: occurrence-config
        mountPath: /etc/gbif
      - name: hadoop-env
        mountPath: /etc/hadoop/conf/core-site.xml
        subPath: core-site.xml
      - name: hadoop-env
        mountPath: /etc/hadoop/conf/hdfs-site.xml
        subPath: hdfs-site.xml
      - name: hive-env
        mountPath: /etc/hadoop/conf/hive-site.xml
        subPath: hive-site.xml
  executor:
    instances: 10
    resources:
      cpu:
        min: "100m"
        max: "10000m"
      memory:
        limit: "8Gi"
    # Mount the GBIF-managed volumes providing environment configuration for Spark
    volumeMounts:
      - name: hadoop-env
        mountPath: /etc/hadoop/conf/core-site.xml
        subPath: core-site.xml
      - name: hadoop-env
        mountPath: /etc/hadoop/conf/hdfs-site.xml
        subPath: hdfs-site.xml
      - name: hive-env
        mountPath: /etc/hadoop/conf/hive-site.xml
        subPath: hive-site.xml
